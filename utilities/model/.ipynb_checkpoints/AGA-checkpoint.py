import random
import numpy as np
from deap import base, creator, tools
from keras.models import Sequential
from keras.optimizers import Adam
from keras.datasets import mnist
from sklearn.model_selection import train_test_split
import tensorflow as tf

# Step 1: Define fitness and individual structures
creator.create("FitnessMax", base.Fitness, weights=(1.0,))
creator.create("Individual", list, fitness=creator.FitnessMax)

# Step 2: Create individual with random hyperparameters for CNN
def create_individual():
    return [random.randint(16, 128),   # Number of filters
            random.randint(16, 128),   # Number of filters for the second layer
            random.randint(16, 128),   # Number of filters for the third layer
            random.uniform(0.0001, 0.01),  # Learning rate
            random.randint(1, 3)]      # Number of fully connected layers

toolbox = base.Toolbox()
toolbox.register("individual", tools.initIterate, creator.Individual, create_individual)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)


def create_cnn(individual):
    # Ensure the filter values are positive and valid
    filters_1 = max(16, int(individual[0]))
    filters_2 = max(16, int(individual[1]))
    filters_3 = max(16, int(individual[2]))

    model = Sequential()
    model.add(Conv1D(filters=filters_1, kernel_size=6, activation='relu', padding='same', input_shape=(1244, 1)))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(MaxPooling1D(pool_size=3, strides=2, padding='same'))

    model.add(Conv1D(filters=filters_2, kernel_size=6, activation='relu', padding='same'))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(MaxPooling1D(pool_size=3, strides=2, padding='same'))

    model.add(Conv1D(filters=filters_3, kernel_size=6, activation='relu', padding='same'))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(MaxPooling1D(pool_size=3, strides=2, padding='same'))

    model.add(Flatten())
    model.add(Dense(64, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(2, activation='softmax'))

    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model


# Step 4: Fitness evaluation
def evaluate(individual):
    tf.keras.backend.clear_session()
    # Ensure filters are positive before using them
    individual = [max(16, int(round(i))) if isinstance(i, float) else int(i) for i in individual]
    
    model = create_cnn(individual)
    model.fit(X_train_cnn, y_train_cnn, epochs=3, batch_size=32, verbose=0)
    _, accuracy = model.evaluate(X_test_cnn, y_test_cnn, verbose=0)
    return accuracy,

toolbox.register("evaluate", evaluate)

# Step 5: Genetic operators (Selection, Crossover, Mutation)
toolbox.register("mate", tools.cxTwoPoint)
toolbox.register("mutate", tools.mutFlipBit, indpb=0.05)  # Modify mutation if needed for floats
toolbox.register("select", tools.selTournament, tournsize=3)

# Step 6: Adaptive parameters (same as before, ensuring valid filter values)
# (use the same `adaptive_parameters` and `evolve_population` functions from the previous example)
def adaptive_parameters(gen, max_gen, fitness_values):
    if gen < max_gen // 2:
        crossover_prob = 0.8
        mutation_prob = 0.1
    else:
        if len(fitness_values) > 5 and max(fitness_values[-5:]) == max(fitness_values):
            mutation_prob = 0.3
        else:
            mutation_prob = 0.05
        crossover_prob = 0.6
    return crossover_prob, mutation_prob

def evolve_population(population, generations):
    fitness_values = []
    for gen in range(generations):
        crossover_prob, mutation_prob = adaptive_parameters(gen, generations, fitness_values)

        offspring = toolbox.select(population, len(population))
        offspring = list(map(toolbox.clone, offspring))

        # Apply crossover
        for child1, child2 in zip(offspring[::2], offspring[1::2]):
            if random.random() < crossover_prob:
                toolbox.mate(child1, child2)
                del child1.fitness.values
                del child2.fitness.values

        # Apply mutation
        for mutant in offspring:
            if random.random() < mutation_prob:
                toolbox.mutate(mutant)
                del mutant.fitness.values

        # Evaluate the new population
        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]
        fitnesses = map(toolbox.evaluate, invalid_ind)
        for ind, fit in zip(invalid_ind, fitnesses):
            ind.fitness.values = fit

        population[:] = offspring

        # Store fitness values
        fits = [ind.fitness.values[0] for ind in population]
        fitness_values.append(max(fits))

        print(f"Generation {gen}: Max Fitness = {max(fits)}")

# Step 7: Initialize and evolve population
population = toolbox.population(n=3)
evolve_population(population, generations=3)

# Retrieve and print the best individual (optimized hyperparameters)
best_individual = tools.selBest(population, k=10)[0]
print(f"Optimized Hyperparameters (Best Individual):")
print(f"Number of Filters (Layer 1): {int(best_individual[0])}")
print(f"Number of Filters (Layer 2): {int(best_individual[1])}")
print(f"Number of Filters (Layer 3): {int(best_individual[2])}")
print(f"Learning Rate: {best_individual[3]:.6f}")